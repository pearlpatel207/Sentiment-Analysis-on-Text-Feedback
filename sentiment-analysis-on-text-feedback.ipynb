{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amazonr/Prime_Pantry_5.json\n",
      "/kaggle/input/dmusic/Digital_Music_5.json\n",
      "/kaggle/input/mixedd/mixed5.csv\n",
      "/kaggle/input/amazonrmix/Gift_Cards_5.json\n",
      "/kaggle/input/amazonrmix/AMAZON_FASHION_5.json\n",
      "/kaggle/input/amazonrmix/Luxury_Beauty_5.json\n",
      "/kaggle/input/amazonrmix/Magazine_Subscriptions_5.json\n",
      "/kaggle/input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import nltk, re, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = open(path)\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df1 = getDF('../input/dmusic/Digital_Music_5.json')\n",
    "df2 = getDF('../input/amazonr/Prime_Pantry_5.json')\n",
    "df3 = getDF('../input/amazonrmix/Luxury_Beauty_5.json')\n",
    "df4 = getDF('../input/amazonrmix/Gift_Cards_5.json')\n",
    "df5 = getDF('../input/amazonrmix/AMAZON_FASHION_5.json')\n",
    "df6 = getDF('../input/amazonrmix/Magazine_Subscriptions_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "f = [df1,df2, df3, df4, df5, df6]\n",
    "df = pd.concat(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350370"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['overall','reviewText']]\n",
    "df['reviewText'].fillna('',inplace=True)\n",
    "df['overall'] = df['overall']/5.0\n",
    "\n",
    "sentences = df['reviewText'].tolist()\n",
    "labels = df['overall'].tolist()\n",
    "len(sentences)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350370"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6782"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]>0.6:\n",
    "        labels[i] = 3\n",
    "    elif labels[i]>0.3:\n",
    "        labels[i] = 2\n",
    "    else:\n",
    "        labels[i] = 1\n",
    "labels.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350365</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350366</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350370 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1  2  3\n",
       "0       0  0  1\n",
       "1       0  0  1\n",
       "2       0  0  1\n",
       "3       0  0  1\n",
       "4       0  0  1\n",
       "...    .. .. ..\n",
       "350365  1  0  0\n",
       "350366  0  0  1\n",
       "350367  0  0  1\n",
       "350368  0  0  1\n",
       "350369  0  0  1\n",
       "\n",
       "[350370 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = pd.get_dummies(labels)\n",
    "labels = ohe\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "data = []\n",
    "for review in sentences:\n",
    "    data.append(clean_text(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280296\n",
      "280296\n"
     ]
    }
   ],
   "source": [
    "training_portion = .8\n",
    "train_size = int(len(data) * training_portion)\n",
    "\n",
    "train_clean = data[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "\n",
    "test_clean = data[train_size:]\n",
    "test_labels = labels[train_size:]\n",
    "\n",
    "print(len(train_clean))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews\n",
    "all_reviews = train_clean + test_clean\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_clean)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 75059\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"Words in index: %d\" % len(word_index))\n",
    "max_review_length = 100\n",
    "\n",
    "train_pad = pad_sequences(train_seq, maxlen = max_review_length)\n",
    "test_pad = pad_sequences(test_seq, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_valid, y_train, y_valid = train_test_split(train_pad, train_labels, test_size = 0.15, random_state = 2)\n",
    "x_test = test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42045, 100)\n",
      "42045\n"
     ]
    }
   ],
   "source": [
    "print(x_valid.shape)\n",
    "print(len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt'\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3242: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path=\"early_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     2000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 100)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 200)      60200       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 96, 200)      120200      conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 94, 256)      153856      conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 92, 256)      196864      conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 45, 512)      393728      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 512)          2099200     conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 512)          2099200     conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           65600       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            195         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,189,043\n",
      "Trainable params: 7,189,043\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "epochs = 50\n",
    "embed_size = 100\n",
    "\n",
    "def lstm_model(conv_layers = 2, max_dilation_rate = 3):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    prefilt = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    x = prefilt\n",
    "    for strides in [1, 1, 2]:\n",
    "        x = Conv1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x_f = LSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n",
    "    x_b = LSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x = concatenate([x_f, x_b])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(3, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "lstm_model = lstm_model()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 190600 samples, validate on 47651 samples\n",
      "Epoch 1/50\n",
      "190600/190600 [==============================] - 72s 379us/step - loss: 0.3468 - categorical_accuracy: 0.9089 - val_loss: 0.2997 - val_categorical_accuracy: 0.9180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29968, saving model to early_weights.hdf5\n",
      "Epoch 2/50\n",
      "190600/190600 [==============================] - 66s 346us/step - loss: 0.2942 - categorical_accuracy: 0.9175 - val_loss: 0.2714 - val_categorical_accuracy: 0.9183\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.29968 to 0.27139, saving model to early_weights.hdf5\n",
      "Epoch 3/50\n",
      "190600/190600 [==============================] - 66s 346us/step - loss: 0.2715 - categorical_accuracy: 0.9194 - val_loss: 0.2545 - val_categorical_accuracy: 0.9234\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.27139 to 0.25446, saving model to early_weights.hdf5\n",
      "Epoch 4/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2566 - categorical_accuracy: 0.9230 - val_loss: 0.2474 - val_categorical_accuracy: 0.9253\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25446 to 0.24740, saving model to early_weights.hdf5\n",
      "Epoch 5/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2465 - categorical_accuracy: 0.9259 - val_loss: 0.2537 - val_categorical_accuracy: 0.9261\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24740\n",
      "Epoch 6/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2403 - categorical_accuracy: 0.9278 - val_loss: 0.2451 - val_categorical_accuracy: 0.9269\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24740 to 0.24506, saving model to early_weights.hdf5\n",
      "Epoch 7/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2342 - categorical_accuracy: 0.9299 - val_loss: 0.2439 - val_categorical_accuracy: 0.9288\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24506 to 0.24389, saving model to early_weights.hdf5\n",
      "Epoch 8/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2291 - categorical_accuracy: 0.9308 - val_loss: 0.2425 - val_categorical_accuracy: 0.9275\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24389 to 0.24248, saving model to early_weights.hdf5\n",
      "Epoch 9/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2233 - categorical_accuracy: 0.9329 - val_loss: 0.2374 - val_categorical_accuracy: 0.9274\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.24248 to 0.23740, saving model to early_weights.hdf5\n",
      "Epoch 10/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2198 - categorical_accuracy: 0.9342 - val_loss: 0.2374 - val_categorical_accuracy: 0.9283\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23740\n",
      "Epoch 11/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2168 - categorical_accuracy: 0.9353 - val_loss: 0.2402 - val_categorical_accuracy: 0.9298\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.23740\n",
      "Epoch 12/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2116 - categorical_accuracy: 0.9363 - val_loss: 0.2382 - val_categorical_accuracy: 0.9298\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23740\n",
      "Epoch 13/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2078 - categorical_accuracy: 0.9373 - val_loss: 0.2355 - val_categorical_accuracy: 0.9301\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.23740 to 0.23549, saving model to early_weights.hdf5\n",
      "Epoch 14/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2050 - categorical_accuracy: 0.9384 - val_loss: 0.2538 - val_categorical_accuracy: 0.9207\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.23549\n",
      "Epoch 15/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.2031 - categorical_accuracy: 0.9392 - val_loss: 0.2391 - val_categorical_accuracy: 0.9290\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.23549\n",
      "Epoch 16/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.1994 - categorical_accuracy: 0.9409 - val_loss: 0.2520 - val_categorical_accuracy: 0.9306\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.23549\n",
      "Epoch 17/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.1943 - categorical_accuracy: 0.9427 - val_loss: 0.2511 - val_categorical_accuracy: 0.9300\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.23549\n",
      "Epoch 18/50\n",
      "190600/190600 [==============================] - 66s 347us/step - loss: 0.1943 - categorical_accuracy: 0.9426 - val_loss: 0.2466 - val_categorical_accuracy: 0.9267\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.23549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f13b485c128>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70074/70074 [==============================] - 7s 99us/step\n",
      "Test score: 0.4356846055019377\n",
      "Test accuracy: 0.8537545800209045\n"
     ]
    }
   ],
   "source": [
    "#6 domains into 3 categories : Good, Average, Bad\n",
    "lstm_model.load_weights(weight_path)\n",
    "score, acc = lstm_model.evaluate(x_test, test_labels, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
